#!/usr/bin/env python3
"""
Windows .exe Packaging Backend Test Suite for PGY3-HUB FastAPI Backend

This test suite specifically validates the backend for Windows .exe packaging with PyInstaller.
Tests focus on offline operation, file storage, CORS, and desktop app compatibility.

Key Testing Areas:
1. API Endpoints - All mind map CRUD operations
2. Data Persistence - JSON file storage for offline operation  
3. File Upload - PDF upload functionality
4. CORS Configuration - Localhost CORS for Electron
5. Error Handling - Graceful error handling and fallbacks
6. Performance - API response times for desktop app
"""

import requests
import json
import uuid
import time
import os
import tempfile
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, List

# Use the production backend URL from frontend/.env
BASE_URL = "https://336d4c80-d84e-4815-a915-e2ffd980488a.preview.emergentagent.com/api"

class WindowsExeBackendTester:
    def __init__(self, base_url: str):
        self.base_url = base_url
        self.test_results = {
            "total": 0,
            "passed": 0,
            "failed": 0,
            "critical_failures": [],
            "performance_metrics": {},
            "details": []
        }
        self.start_time = None
        
    def run_test(self, name: str, test_func, is_critical: bool = False):
        """Run a test and record the result"""
        self.test_results["total"] += 1
        print(f"\nüîç Testing: {name}")
        
        start_time = time.time()
        try:
            result = test_func()
            end_time = time.time()
            duration = end_time - start_time
            
            if result:
                self.test_results["passed"] += 1
                status = f"‚úÖ PASSED ({duration:.3f}s)"
            else:
                self.test_results["failed"] += 1
                status = f"‚ùå FAILED ({duration:.3f}s)"
                if is_critical:
                    self.test_results["critical_failures"].append(name)
        except Exception as e:
            end_time = time.time()
            duration = end_time - start_time
            self.test_results["failed"] += 1
            status = f"‚ùå ERROR: {str(e)} ({duration:.3f}s)"
            result = False
            if is_critical:
                self.test_results["critical_failures"].append(name)
            
        self.test_results["details"].append({
            "name": name,
            "status": status,
            "duration": duration,
            "critical": is_critical
        })
        
        print(f"{status}")
        return result
    
    def test_api_connectivity(self):
        """Test basic API connectivity and root endpoint"""
        try:
            response = requests.get(f"{self.base_url}/", timeout=10)
            if response.status_code != 200:
                print(f"  Expected status code 200, got {response.status_code}")
                return False
            
            data = response.json()
            if "message" not in data:
                print(f"  Expected message field in response, got {data}")
                return False
                
            print(f"  API Response: {data['message']}")
            return True
        except requests.exceptions.RequestException as e:
            print(f"  Connection error: {e}")
            return False
    
    def test_mindmap_data_retrieval(self):
        """Test GET /api/mindmap-data endpoint for data retrieval"""
        try:
            response = requests.get(f"{self.base_url}/mindmap-data", timeout=10)
            if response.status_code != 200:
                print(f"  Expected status code 200, got {response.status_code}")
                return False
            
            data = response.json()
            required_keys = ["topics", "cases", "tasks", "literature", "connections"]
            
            for key in required_keys:
                if key not in data:
                    print(f"  Missing required key '{key}' in response")
                    return False
            
            # Store data for later tests
            self.mindmap_data = data
            
            # Print data summary
            print(f"  Retrieved: {len(data['topics'])} topics, {len(data['cases'])} cases, "
                  f"{len(data['tasks'])} tasks, {len(data['literature'])} literature, "
                  f"{len(data['connections'])} connections")
            
            return True
        except requests.exceptions.RequestException as e:
            print(f"  Request error: {e}")
            return False
    
    def test_data_persistence_save(self):
        """Test PUT /api/mindmap-data endpoint for data persistence"""
        if not hasattr(self, 'mindmap_data'):
            print("  No mindmap data available for testing")
            return False
        
        try:
            # Create a test topic to verify save functionality
            test_topic = {
                "id": f"test-topic-{uuid.uuid4().hex[:8]}",
                "title": "Windows .exe Test Topic",
                "description": "Test topic for Windows executable packaging validation",
                "category": "Test Category",
                "color": "#FF6B6B",
                "position": {"x": 100, "y": 100},
                "flashcard_count": 0,
                "completed_flashcards": 0,
                "resources": [],
                "notes": "Created during Windows .exe backend testing",
                "tags": "test,windows,exe",
                "definition": "Test definition for packaging validation",
                "diagnostic_criteria": ["Test criteria 1", "Test criteria 2"],
                "comorbidities": ["Test comorbidity"],
                "differential_diagnoses": ["Test differential"],
                "medications": [],
                "psychotherapy_modalities": [],
                "created_at": datetime.utcnow().isoformat(),
                "updated_at": datetime.utcnow().isoformat()
            }
            
            # Add test topic to data
            modified_data = self.mindmap_data.copy()
            modified_data["topics"].append(test_topic)
            
            # Save the data
            response = requests.put(f"{self.base_url}/mindmap-data", 
                                  json=modified_data, timeout=15)
            
            if response.status_code != 200:
                print(f"  Save failed with status code {response.status_code}")
                return False
            
            # Store the test topic ID for verification
            self.test_topic_id = test_topic["id"]
            print(f"  Successfully saved data with test topic: {test_topic['title']}")
            
            return True
        except requests.exceptions.RequestException as e:
            print(f"  Request error during save: {e}")
            return False
    
    def test_data_persistence_verification(self):
        """Verify that saved data persists correctly"""
        if not hasattr(self, 'test_topic_id'):
            print("  No test topic ID available for verification")
            return False
        
        try:
            # Retrieve data again to verify persistence
            response = requests.get(f"{self.base_url}/mindmap-data", timeout=10)
            if response.status_code != 200:
                print(f"  Verification failed with status code {response.status_code}")
                return False
            
            data = response.json()
            
            # Look for our test topic
            test_topic_found = False
            for topic in data["topics"]:
                if topic["id"] == self.test_topic_id:
                    test_topic_found = True
                    print(f"  Test topic found: {topic['title']}")
                    break
            
            if not test_topic_found:
                print(f"  Test topic with ID {self.test_topic_id} not found in retrieved data")
                return False
            
            print(f"  Data persistence verified successfully")
            return True
        except requests.exceptions.RequestException as e:
            print(f"  Request error during verification: {e}")
            return False
    
    def test_crud_operations_topics(self):
        """Test CRUD operations for topics"""
        try:
            # Test creating a new topic via bulk update
            new_topic = {
                "id": f"crud-topic-{uuid.uuid4().hex[:8]}",
                "title": "CRUD Test Topic",
                "description": "Topic created for CRUD testing",
                "category": "CRUD Testing",
                "color": "#4ECDC4",
                "position": {"x": 200, "y": 200},
                "flashcard_count": 5,
                "completed_flashcards": 2,
                "resources": [{"title": "Test Resource", "url": "#", "type": "reference"}],
                "notes": "CRUD test notes",
                "tags": "crud,test",
                "created_at": datetime.utcnow().isoformat(),
                "updated_at": datetime.utcnow().isoformat()
            }
            
            # Get current data
            get_response = requests.get(f"{self.base_url}/mindmap-data", timeout=10)
            if get_response.status_code != 200:
                print(f"  Failed to get current data: {get_response.status_code}")
                return False
            
            current_data = get_response.json()
            current_data["topics"].append(new_topic)
            
            # Save updated data
            put_response = requests.put(f"{self.base_url}/mindmap-data", 
                                      json=current_data, timeout=15)
            if put_response.status_code != 200:
                print(f"  Failed to create topic: {put_response.status_code}")
                return False
            
            # Verify creation
            verify_response = requests.get(f"{self.base_url}/mindmap-data", timeout=10)
            if verify_response.status_code != 200:
                print(f"  Failed to verify creation: {verify_response.status_code}")
                return False
            
            verified_data = verify_response.json()
            topic_found = any(topic["id"] == new_topic["id"] for topic in verified_data["topics"])
            
            if not topic_found:
                print(f"  Created topic not found in verification")
                return False
            
            # Test updating the topic
            for topic in verified_data["topics"]:
                if topic["id"] == new_topic["id"]:
                    topic["title"] = "Updated CRUD Test Topic"
                    topic["description"] = "Updated description for CRUD testing"
                    topic["updated_at"] = datetime.utcnow().isoformat()
                    break
            
            # Save updated data
            update_response = requests.put(f"{self.base_url}/mindmap-data", 
                                         json=verified_data, timeout=15)
            if update_response.status_code != 200:
                print(f"  Failed to update topic: {update_response.status_code}")
                return False
            
            # Verify update
            final_response = requests.get(f"{self.base_url}/mindmap-data", timeout=10)
            if final_response.status_code != 200:
                print(f"  Failed to verify update: {final_response.status_code}")
                return False
            
            final_data = final_response.json()
            updated_topic = next((topic for topic in final_data["topics"] 
                                if topic["id"] == new_topic["id"]), None)
            
            if not updated_topic or updated_topic["title"] != "Updated CRUD Test Topic":
                print(f"  Topic update not verified")
                return False
            
            # Store for cleanup
            self.crud_topic_id = new_topic["id"]
            
            print(f"  Topic CRUD operations successful")
            return True
            
        except requests.exceptions.RequestException as e:
            print(f"  Request error during CRUD operations: {e}")
            return False
    
    def test_crud_operations_cases(self):
        """Test CRUD operations for patient cases"""
        try:
            # Create a new case
            new_case = {
                "id": f"crud-case-{uuid.uuid4().hex[:8]}",
                "case_id": "CRUD-TEST-001",
                "encounter_date": datetime.utcnow().isoformat(),
                "primary_diagnosis": "Test Diagnosis for CRUD",
                "secondary_diagnoses": ["Secondary Test Diagnosis"],
                "age": 30,
                "gender": "Test",
                "chief_complaint": "CRUD testing complaint",
                "initial_presentation": "Test presentation",
                "current_presentation": "Current test presentation",
                "medication_history": "Test medication history",
                "therapy_progress": "Test therapy progress",
                "defense_patterns": "Test defense patterns",
                "clinical_reflection": "Test clinical reflection",
                "history_present_illness": "Test HPI",
                "medical_history": "Test medical history",
                "medications": ["Test Medication 1", "Test Medication 2"],
                "mental_status_exam": "Test MSE",
                "assessment_plan": "Test assessment and plan",
                "notes": "CRUD test case notes",
                "status": "active",
                "linked_topics": [],
                "position": {"x": 300, "y": 300},
                "timeline": [
                    {
                        "id": f"timeline-{uuid.uuid4().hex[:8]}",
                        "type": "Assessment",
                        "timestamp": datetime.utcnow().isoformat(),
                        "content": "Initial CRUD test assessment",
                        "patient_narrative": "Test patient narrative",
                        "clinical_notes": "Test clinical notes",
                        "author": "CRUD Tester",
                        "metadata": {"test": True}
                    }
                ],
                "created_at": datetime.utcnow().isoformat(),
                "updated_at": datetime.utcnow().isoformat()
            }
            
            # Get current data and add case
            get_response = requests.get(f"{self.base_url}/mindmap-data", timeout=10)
            if get_response.status_code != 200:
                print(f"  Failed to get current data: {get_response.status_code}")
                return False
            
            current_data = get_response.json()
            current_data["cases"].append(new_case)
            
            # Save data
            put_response = requests.put(f"{self.base_url}/mindmap-data", 
                                      json=current_data, timeout=15)
            if put_response.status_code != 200:
                print(f"  Failed to create case: {put_response.status_code}")
                return False
            
            # Verify and update case
            verify_response = requests.get(f"{self.base_url}/mindmap-data", timeout=10)
            verified_data = verify_response.json()
            
            case_found = False
            for case in verified_data["cases"]:
                if case["id"] == new_case["id"]:
                    case_found = True
                    # Update case
                    case["primary_diagnosis"] = "Updated Test Diagnosis"
                    case["timeline"].append({
                        "id": f"timeline-{uuid.uuid4().hex[:8]}",
                        "type": "Follow-up",
                        "timestamp": datetime.utcnow().isoformat(),
                        "content": "Updated CRUD test follow-up",
                        "author": "CRUD Tester",
                        "metadata": {"updated": True}
                    })
                    break
            
            if not case_found:
                print(f"  Created case not found")
                return False
            
            # Save updated case
            update_response = requests.put(f"{self.base_url}/mindmap-data", 
                                         json=verified_data, timeout=15)
            if update_response.status_code != 200:
                print(f"  Failed to update case: {update_response.status_code}")
                return False
            
            # Store for cleanup
            self.crud_case_id = new_case["id"]
            
            print(f"  Case CRUD operations successful (including timeline)")
            return True
            
        except requests.exceptions.RequestException as e:
            print(f"  Request error during case CRUD: {e}")
            return False
    
    def test_crud_operations_tasks(self):
        """Test CRUD operations for tasks"""
        try:
            new_task = {
                "id": f"crud-task-{uuid.uuid4().hex[:8]}",
                "title": "CRUD Test Task",
                "description": "Task created for CRUD testing",
                "status": "pending",
                "priority": "high",
                "due_date": (datetime.utcnow()).isoformat(),
                "linked_case_id": None,
                "linked_topic_id": None,
                "notes": "CRUD test task notes",
                "position": {"x": 400, "y": 400},
                "created_at": datetime.utcnow().isoformat(),
                "updated_at": datetime.utcnow().isoformat()
            }
            
            # Get, add, save, verify pattern
            get_response = requests.get(f"{self.base_url}/mindmap-data", timeout=10)
            current_data = get_response.json()
            current_data["tasks"].append(new_task)
            
            put_response = requests.put(f"{self.base_url}/mindmap-data", 
                                      json=current_data, timeout=15)
            if put_response.status_code != 200:
                print(f"  Failed to create task: {put_response.status_code}")
                return False
            
            # Update task status
            verify_response = requests.get(f"{self.base_url}/mindmap-data", timeout=10)
            verified_data = verify_response.json()
            
            for task in verified_data["tasks"]:
                if task["id"] == new_task["id"]:
                    task["status"] = "completed"
                    task["notes"] = "Updated CRUD test task - completed"
                    break
            
            update_response = requests.put(f"{self.base_url}/mindmap-data", 
                                         json=verified_data, timeout=15)
            if update_response.status_code != 200:
                print(f"  Failed to update task: {update_response.status_code}")
                return False
            
            self.crud_task_id = new_task["id"]
            print(f"  Task CRUD operations successful")
            return True
            
        except requests.exceptions.RequestException as e:
            print(f"  Request error during task CRUD: {e}")
            return False
    
    def test_crud_operations_literature(self):
        """Test CRUD operations for literature"""
        try:
            new_literature = {
                "id": f"crud-lit-{uuid.uuid4().hex[:8]}",
                "title": "CRUD Test Literature",
                "authors": "Test Author, Another Author",
                "publication": "Journal of CRUD Testing",
                "year": 2024,
                "doi": "10.1000/crud.test.2024",
                "abstract": "This is a test abstract for CRUD operations validation in the Windows .exe packaging process.",
                "notes": "CRUD test literature notes",
                "pdf_path": None,
                "linked_topics": [],
                "position": {"x": 500, "y": 500},
                "created_at": datetime.utcnow().isoformat(),
                "updated_at": datetime.utcnow().isoformat()
            }
            
            # Standard CRUD pattern
            get_response = requests.get(f"{self.base_url}/mindmap-data", timeout=10)
            current_data = get_response.json()
            current_data["literature"].append(new_literature)
            
            put_response = requests.put(f"{self.base_url}/mindmap-data", 
                                      json=current_data, timeout=15)
            if put_response.status_code != 200:
                print(f"  Failed to create literature: {put_response.status_code}")
                return False
            
            # Update literature
            verify_response = requests.get(f"{self.base_url}/mindmap-data", timeout=10)
            verified_data = verify_response.json()
            
            for lit in verified_data["literature"]:
                if lit["id"] == new_literature["id"]:
                    lit["notes"] = "Updated CRUD test literature notes"
                    lit["abstract"] = "Updated abstract for CRUD testing validation"
                    break
            
            update_response = requests.put(f"{self.base_url}/mindmap-data", 
                                         json=verified_data, timeout=15)
            if update_response.status_code != 200:
                print(f"  Failed to update literature: {update_response.status_code}")
                return False
            
            self.crud_literature_id = new_literature["id"]
            print(f"  Literature CRUD operations successful")
            return True
            
        except requests.exceptions.RequestException as e:
            print(f"  Request error during literature CRUD: {e}")
            return False
    
    def test_pdf_upload_functionality(self):
        """Test PDF upload functionality for literature nodes"""
        try:
            # Create a temporary PDF file for testing
            with tempfile.NamedTemporaryFile(suffix='.pdf', delete=False) as temp_pdf:
                # Write some PDF-like content (minimal PDF structure)
                pdf_content = b"""%PDF-1.4
1 0 obj
<<
/Type /Catalog
/Pages 2 0 R
>>
endobj
2 0 obj
<<
/Type /Pages
/Kids [3 0 R]
/Count 1
>>
endobj
3 0 obj
<<
/Type /Page
/Parent 2 0 R
/MediaBox [0 0 612 792]
>>
endobj
xref
0 4
0000000000 65535 f 
0000000010 00000 n 
0000000053 00000 n 
0000000125 00000 n 
trailer
<<
/Size 4
/Root 1 0 R
>>
startxref
229
%%EOF"""
                temp_pdf.write(pdf_content)
                temp_pdf_path = temp_pdf.name
            
            try:
                # Test PDF upload
                with open(temp_pdf_path, 'rb') as pdf_file:
                    files = {'pdf': ('test_document.pdf', pdf_file, 'application/pdf')}
                    response = requests.post(f"{self.base_url}/upload-pdf", 
                                           files=files, timeout=30)
                
                if response.status_code != 200:
                    print(f"  PDF upload failed with status code {response.status_code}")
                    if response.status_code == 404:
                        print(f"  Upload endpoint not found - may not be implemented")
                        return True  # Not a critical failure for .exe packaging
                    return False
                
                upload_result = response.json()
                
                if "filePath" not in upload_result:
                    print(f"  Upload response missing filePath: {upload_result}")
                    return False
                
                file_path = upload_result["filePath"]
                print(f"  PDF uploaded successfully: {file_path}")
                
                # Verify the uploaded file can be accessed
                file_url = f"{self.base_url.replace('/api', '')}{file_path}"
                verify_response = requests.head(file_url, timeout=10)
                
                if verify_response.status_code == 200:
                    print(f"  Uploaded PDF is accessible at: {file_url}")
                else:
                    print(f"  Uploaded PDF not accessible (status: {verify_response.status_code})")
                    # This might be expected in some deployment configurations
                
                return True
                
            finally:
                # Clean up temporary file
                if os.path.exists(temp_pdf_path):
                    os.unlink(temp_pdf_path)
                    
        except requests.exceptions.RequestException as e:
            print(f"  Request error during PDF upload: {e}")
            return False
        except Exception as e:
            print(f"  Error during PDF upload test: {e}")
            return False
    
    def test_cors_configuration(self):
        """Test CORS configuration for Electron desktop app"""
        try:
            # Test CORS with localhost:3000 (typical Electron setup)
            cors_origins = ["http://localhost:3000", "http://localhost:3001"]
            
            for origin in cors_origins:
                # Send OPTIONS request to check CORS
                headers = {
                    "Origin": origin,
                    "Access-Control-Request-Method": "GET",
                    "Access-Control-Request-Headers": "Content-Type"
                }
                
                response = requests.options(f"{self.base_url}/mindmap-data", 
                                          headers=headers, timeout=10)
                
                # Check CORS headers
                cors_origin = response.headers.get("Access-Control-Allow-Origin")
                cors_methods = response.headers.get("Access-Control-Allow-Methods")
                
                if cors_origin:
                    print(f"  CORS configured for origin {origin}: {cors_origin}")
                else:
                    print(f"  No CORS headers found for origin {origin}")
                
                # Test actual GET request with CORS
                get_response = requests.get(f"{self.base_url}/mindmap-data", 
                                          headers={"Origin": origin}, timeout=10)
                
                if get_response.status_code == 200:
                    print(f"  GET request successful with origin {origin}")
                else:
                    print(f"  GET request failed with origin {origin}: {get_response.status_code}")
            
            print(f"  CORS configuration tested for Electron compatibility")
            return True
            
        except requests.exceptions.RequestException as e:
            print(f"  Request error during CORS test: {e}")
            return False
    
    def test_error_handling(self):
        """Test graceful error handling and fallbacks"""
        try:
            # Test 1: Invalid JSON data
            invalid_data = {"invalid": "structure", "missing": "required_fields"}
            
            response = requests.put(f"{self.base_url}/mindmap-data", 
                                  json=invalid_data, timeout=15)
            
            if response.status_code >= 400:
                print(f"  Invalid data correctly rejected (status: {response.status_code})")
            else:
                print(f"  Invalid data accepted - server is lenient (status: {response.status_code})")
            
            # Test 2: Malformed JSON
            try:
                malformed_response = requests.put(f"{self.base_url}/mindmap-data", 
                                                data="invalid json content", 
                                                headers={"Content-Type": "application/json"},
                                                timeout=15)
                print(f"  Malformed JSON handled (status: {malformed_response.status_code})")
            except:
                print(f"  Malformed JSON caused connection error (expected)")
            
            # Test 3: Large data handling
            large_data = {
                "topics": [{"id": f"large-topic-{i}", "title": f"Large Topic {i}", 
                           "description": "x" * 1000, "category": "Large", "color": "#000000",
                           "position": {"x": i, "y": i}} for i in range(50)],
                "cases": [],
                "tasks": [],
                "literature": [],
                "connections": []
            }
            
            large_response = requests.put(f"{self.base_url}/mindmap-data", 
                                        json=large_data, timeout=30)
            
            if large_response.status_code == 200:
                print(f"  Large data handled successfully")
            else:
                print(f"  Large data handling failed (status: {large_response.status_code})")
            
            # Test 4: Non-existent endpoint
            nonexistent_response = requests.get(f"{self.base_url}/nonexistent-endpoint", 
                                              timeout=10)
            
            if nonexistent_response.status_code == 404:
                print(f"  Non-existent endpoint correctly returns 404")
            else:
                print(f"  Non-existent endpoint returns: {nonexistent_response.status_code}")
            
            print(f"  Error handling tests completed")
            return True
            
        except requests.exceptions.RequestException as e:
            print(f"  Request error during error handling test: {e}")
            return False
    
    def test_performance_metrics(self):
        """Test API response times for desktop app suitability"""
        try:
            performance_results = {}
            
            # Test 1: GET mindmap-data performance
            get_times = []
            for i in range(5):
                start_time = time.time()
                response = requests.get(f"{self.base_url}/mindmap-data", timeout=10)
                end_time = time.time()
                
                if response.status_code == 200:
                    get_times.append(end_time - start_time)
                else:
                    print(f"  GET request failed in performance test")
                    return False
            
            avg_get_time = sum(get_times) / len(get_times)
            performance_results["avg_get_time"] = avg_get_time
            
            # Test 2: PUT mindmap-data performance
            if hasattr(self, 'mindmap_data'):
                put_times = []
                for i in range(3):
                    start_time = time.time()
                    response = requests.put(f"{self.base_url}/mindmap-data", 
                                          json=self.mindmap_data, timeout=15)
                    end_time = time.time()
                    
                    if response.status_code == 200:
                        put_times.append(end_time - start_time)
                    else:
                        print(f"  PUT request failed in performance test")
                        return False
                
                avg_put_time = sum(put_times) / len(put_times)
                performance_results["avg_put_time"] = avg_put_time
            
            # Store performance metrics
            self.test_results["performance_metrics"] = performance_results
            
            print(f"  Average GET time: {avg_get_time:.3f}s")
            if "avg_put_time" in performance_results:
                print(f"  Average PUT time: {performance_results['avg_put_time']:.3f}s")
            
            # Check if performance is suitable for desktop app
            if avg_get_time > 5.0:
                print(f"  WARNING: GET response time may be too slow for desktop app")
            
            if "avg_put_time" in performance_results and performance_results["avg_put_time"] > 10.0:
                print(f"  WARNING: PUT response time may be too slow for desktop app")
            
            print(f"  Performance metrics collected successfully")
            return True
            
        except requests.exceptions.RequestException as e:
            print(f"  Request error during performance test: {e}")
            return False
    
    def test_data_integrity(self):
        """Test data integrity across multiple operations"""
        try:
            # Get initial data
            initial_response = requests.get(f"{self.base_url}/mindmap-data", timeout=10)
            if initial_response.status_code != 200:
                print(f"  Failed to get initial data: {initial_response.status_code}")
                return False
            
            initial_data = initial_response.json()
            initial_counts = {
                "topics": len(initial_data["topics"]),
                "cases": len(initial_data["cases"]),
                "tasks": len(initial_data["tasks"]),
                "literature": len(initial_data["literature"]),
                "connections": len(initial_data["connections"])
            }
            
            # Perform multiple operations
            for i in range(3):
                # Add a test item
                test_topic = {
                    "id": f"integrity-topic-{i}-{uuid.uuid4().hex[:8]}",
                    "title": f"Integrity Test Topic {i}",
                    "description": f"Topic for data integrity test iteration {i}",
                    "category": "Integrity Test",
                    "color": "#9B59B6",
                    "position": {"x": 100 + i * 50, "y": 100 + i * 50},
                    "created_at": datetime.utcnow().isoformat(),
                    "updated_at": datetime.utcnow().isoformat()
                }
                
                current_data = initial_data.copy()
                current_data["topics"].append(test_topic)
                
                # Save data
                save_response = requests.put(f"{self.base_url}/mindmap-data", 
                                           json=current_data, timeout=15)
                if save_response.status_code != 200:
                    print(f"  Failed to save in iteration {i}: {save_response.status_code}")
                    return False
                
                # Verify data
                verify_response = requests.get(f"{self.base_url}/mindmap-data", timeout=10)
                if verify_response.status_code != 200:
                    print(f"  Failed to verify in iteration {i}: {verify_response.status_code}")
                    return False
                
                verified_data = verify_response.json()
                
                # Check that our topic exists
                topic_found = any(topic["id"] == test_topic["id"] 
                                for topic in verified_data["topics"])
                
                if not topic_found:
                    print(f"  Topic not found in iteration {i}")
                    return False
                
                # Update initial_data for next iteration
                initial_data = verified_data
            
            # Final verification
            final_response = requests.get(f"{self.base_url}/mindmap-data", timeout=10)
            final_data = final_response.json()
            
            final_counts = {
                "topics": len(final_data["topics"]),
                "cases": len(final_data["cases"]),
                "tasks": len(final_data["tasks"]),
                "literature": len(final_data["literature"]),
                "connections": len(final_data["connections"])
            }
            
            # Check that we added exactly 3 topics
            if final_counts["topics"] != initial_counts["topics"] + 3:
                print(f"  Data integrity issue: expected {initial_counts['topics'] + 3} topics, "
                      f"got {final_counts['topics']}")
                return False
            
            print(f"  Data integrity maintained across multiple operations")
            print(f"  Initial counts: {initial_counts}")
            print(f"  Final counts: {final_counts}")
            
            return True
            
        except requests.exceptions.RequestException as e:
            print(f"  Request error during data integrity test: {e}")
            return False
    
    def cleanup_test_data(self):
        """Clean up test data created during testing"""
        try:
            # Get current data
            response = requests.get(f"{self.base_url}/mindmap-data", timeout=10)
            if response.status_code != 200:
                print(f"  Failed to get data for cleanup: {response.status_code}")
                return False
            
            data = response.json()
            
            # Remove test items
            test_ids = []
            if hasattr(self, 'test_topic_id'):
                test_ids.append(self.test_topic_id)
            if hasattr(self, 'crud_topic_id'):
                test_ids.append(self.crud_topic_id)
            if hasattr(self, 'crud_case_id'):
                test_ids.append(self.crud_case_id)
            if hasattr(self, 'crud_task_id'):
                test_ids.append(self.crud_task_id)
            if hasattr(self, 'crud_literature_id'):
                test_ids.append(self.crud_literature_id)
            
            # Filter out test data
            data["topics"] = [topic for topic in data["topics"] 
                            if not (topic["id"] in test_ids or 
                                   topic["title"].startswith(("Windows .exe Test", "CRUD Test", "Integrity Test")))]
            
            data["cases"] = [case for case in data["cases"] 
                           if not (case["id"] in test_ids or 
                                  case.get("case_id", "").startswith("CRUD-TEST"))]
            
            data["tasks"] = [task for task in data["tasks"] 
                           if not (task["id"] in test_ids or 
                                  task["title"].startswith("CRUD Test"))]
            
            data["literature"] = [lit for lit in data["literature"] 
                                if not (lit["id"] in test_ids or 
                                       lit["title"].startswith("CRUD Test"))]
            
            # Save cleaned data
            cleanup_response = requests.put(f"{self.base_url}/mindmap-data", 
                                          json=data, timeout=15)
            
            if cleanup_response.status_code == 200:
                print(f"  Test data cleanup completed successfully")
                return True
            else:
                print(f"  Test data cleanup failed: {cleanup_response.status_code}")
                return False
                
        except requests.exceptions.RequestException as e:
            print(f"  Request error during cleanup: {e}")
            return False
    
    def run_all_tests(self):
        """Run all tests for Windows .exe packaging validation"""
        print("üöÄ Starting Windows .exe Backend Validation Tests üöÄ")
        print(f"Testing backend at: {self.base_url}")
        
        self.start_time = time.time()
        
        # Critical tests for .exe packaging
        self.run_test("API Connectivity", self.test_api_connectivity, is_critical=True)
        self.run_test("Mind Map Data Retrieval", self.test_mindmap_data_retrieval, is_critical=True)
        self.run_test("Data Persistence - Save", self.test_data_persistence_save, is_critical=True)
        self.run_test("Data Persistence - Verification", self.test_data_persistence_verification, is_critical=True)
        
        # CRUD operations tests
        self.run_test("CRUD Operations - Topics", self.test_crud_operations_topics, is_critical=True)
        self.run_test("CRUD Operations - Cases", self.test_crud_operations_cases, is_critical=True)
        self.run_test("CRUD Operations - Tasks", self.test_crud_operations_tasks, is_critical=True)
        self.run_test("CRUD Operations - Literature", self.test_crud_operations_literature, is_critical=True)
        
        # File upload and CORS tests
        self.run_test("PDF Upload Functionality", self.test_pdf_upload_functionality)
        self.run_test("CORS Configuration", self.test_cors_configuration)
        
        # Error handling and performance
        self.run_test("Error Handling", self.test_error_handling)
        self.run_test("Performance Metrics", self.test_performance_metrics, is_critical=True)
        self.run_test("Data Integrity", self.test_data_integrity, is_critical=True)
        
        # Cleanup
        self.run_test("Test Data Cleanup", self.cleanup_test_data)
        
        total_time = time.time() - self.start_time
        
        # Print comprehensive summary
        print("\n" + "="*80)
        print("üìä WINDOWS .EXE BACKEND VALIDATION SUMMARY")
        print("="*80)
        print(f"Total Tests: {self.test_results['total']}")
        print(f"Passed: {self.test_results['passed']}")
        print(f"Failed: {self.test_results['failed']}")
        print(f"Total Time: {total_time:.3f}s")
        
        if self.test_results["performance_metrics"]:
            print(f"\n‚ö° Performance Metrics:")
            for metric, value in self.test_results["performance_metrics"].items():
                print(f"  {metric}: {value:.3f}s")
        
        if self.test_results["critical_failures"]:
            print(f"\nüö® CRITICAL FAILURES:")
            for failure in self.test_results["critical_failures"]:
                print(f"  ‚ùå {failure}")
        
        if self.test_results['failed'] > 0:
            print(f"\n‚ùå Failed Tests:")
            for detail in self.test_results['details']:
                if "FAILED" in detail['status'] or "ERROR" in detail['status']:
                    critical_marker = " [CRITICAL]" if detail.get('critical') else ""
                    print(f"  - {detail['name']}: {detail['status']}{critical_marker}")
        
        # Final assessment
        critical_failures = len(self.test_results["critical_failures"])
        total_failures = self.test_results['failed']
        
        print(f"\nüéØ WINDOWS .EXE PACKAGING READINESS:")
        
        if critical_failures == 0:
            print("‚úÖ READY FOR PACKAGING - All critical tests passed")
            if total_failures > 0:
                print(f"‚ö†Ô∏è  {total_failures} non-critical issues found (see details above)")
        else:
            print(f"‚ùå NOT READY FOR PACKAGING - {critical_failures} critical failures")
            print("üîß Fix critical issues before proceeding with .exe packaging")
        
        return critical_failures == 0

if __name__ == "__main__":
    tester = WindowsExeBackendTester(BASE_URL)
    success = tester.run_all_tests()
    
    if success:
        print("\nüéâ Backend is ready for Windows .exe packaging!")
    else:
        print("\n‚ö†Ô∏è  Backend needs fixes before Windows .exe packaging!")